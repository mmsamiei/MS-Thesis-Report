\chapter{راهکار پیشنهادی}\label{chap4}
\minitoc


\section{مقدمه}
پس از تعریف مساله و مرور پژوهش‌های پیشین مرتبط با آن و ارائه مبانی نظری، در این  فصل راهکاری پیشنهادی برای مساله گپ‌زن دانش‌بنیان طرح خواهد شد. این طرح شامل ارائه کلیات درباره دو بخش انتخاب دانش و تولید پاسخ است و در جزییات هر یک از این بخش‌ها نیز تشریح می‌شود. اساس این راهکار پیشنهادی، با توجه به شرایط سخت افزاری در دسترس برای پیاده‌سازی آن، بر دو بخش انتخاب دانش و تولید پاسخ بنا شده است که تکیه هر یک از بخش ‌ها بر استفاده از شبکه‌های از پیش آموزش دیده عصاره‌گیری شده و انجام انتقال یادگیری در آن هاست.


\section{ساختار کلی مدل پیشنهادی}
در این پژوهش خلق یک گپ‌زن دانش‌بنیان 
با عنایت به طرح کلی مدل جادوگر ویکی پدیا،
معادل با حل سه زیر مساله در نظر گرفته شده است و ارائه مدل برای این سه زیرمساله حل گشتن مساله اصلی را نتیجه می‌دهد. در واقع گپ‌زن در هر نوبت از یک مکالمه که بایستی پاسخ خود را تولید کند می‌بایست این سه مرحله را طی کند. این سه زیرمساله عبارتند از:

\begin{enumerate}
	\item 
	\textbf{استخراج اسناد مرتبط}
	در ابتدا گپ‌زن باید بتواند با توجه به تاریخچه مکالمه و پاسخ‌های نوبت‌های قبلی خود و طرف مقابلش، اسناد مرتبط را از منبع دانش خود که می‌تواند شامل چند صد هزار سند متنی باشد استخراج کند. این کار به منظور تمرکز بر روی چند سند مرتبط با مکالمه به جای پردازش کل منبع دانش (که طبیعتا غیرممکن است) صورت می‌گیرد. ورودی این بخش تاریخچه گفت‌ و گو و منبع دانش است و خروجی آن نیز اسناد انتخاب شده از منبع دانش است. 
	\item 
	\textbf{انتخاب دانش}
	پس از انجام مرحله قبل،‌اکنون گپ‌زن با تعدادی سند متنی روبرو است که خود شامل پاراگراف‌ها و جملاتی هستند. قاعدتا تمامی این جملات و پاراگراف‌ها در تولید پاسخ نوبت‌ بعدی موثر و مفید نیستند. از این رو گپ‌زن باز بایستی محدوده جستجوی خود را
	با توجه به تاریخچه گفت و گو محدودتر و متمرکزتر کند. 
	ورودی این مرحله، اسناد انتخاب شده در مرحله اول و خروجی آن نیز می‌تواند یک یا چند جمله 
	به عنوان جمله دانش باشد. ضمن این که گپ‌زن می‌بایستی توان این را داشته باشد که در صورتی که مکالمه‌اش نیازی به جمله دانش نداشت، در خروجی این مرحله این مطلب را اعلام کند. 
\footnote{برای مثال در صورتی که مخاطب از گپ‌زن پرسید که "سلام خوبی؟"، گپ‌زن برای پاسخ به این مکالمه نیازی به استفاده از منبع دانش خود ندارد.}
	\item
	\textbf{تولید پاسخ}
	در این گام، گپ‌زن حال بایستی با جمله یا جملات دانشی که در مرحله قبل انتخاب کرده است و با توجه به تاریخچه گفتگو، پاسخ مطلوب را تولید کند. ورودی این بخش جمله دانش و تاریخچه گفتگو هستند و خروجی آن نیز پاسخ گپ‌زن است. 
	
	
\end{enumerate}

	\trans{مدل پایه}{baseline model}
انتخابی برای مقایسه با محصول این پژوهش، مدل جادوگر ویکی‌پدیا (ارائه شده در بخش 
\ref{chap2:intro:wizard}
)
است.
دادگان پایه مورد استفاده در این پژوهش نیز همان دادگان مدل جادوگر ویکی‌پدیا است.

ذکر این نکته ضروری است که دادگان سنجش جادوگر ویکی‌پدیا در دو دسته عرضه شده‌اند. در دسته اول، دادگان تست مکالمه‌هایی انجام‌شده با موضوعاتی هستند که در مجموعه دادگان آموزشی نیز مکالماتی با این موضوعات وجود دارند. در دسته دوم اما، دادگان تست مکالمه‌هایی با موضوع هایی هستند که نسبت به موضوعات دیده‌شده در دادگان تست متفاوت و جدید و دیده نشده هستند. از این پس در این پژوهش مجموعه اول و دوم به ترتیب مجموعه دادگان آشنا و ناآشنا نامگذاری می‌شوند. جداسازی این دو مجموعه از هم در هنگام سنجش، باعث می‌شود تا قدرت تعمیم دهی مدل روی موضوعات تازه نیز سنجیده شود. 


از آنجایی که در روند جمع‌آوری دادگان جادوگر ویکی‌پدیا،‌ عوامل انسانی تنها مجاز به بناکردن جواب خود بر اسناد پیشنهاد‌شده توسط موتور جستجوگر استفاده شده بودند، در این پژوهش نیز
برای حل زیر مساله استخراج اسناد مرتبط
از مدل بازیابی اطلاعاتی 
\lr{DRQA}
استفاده شده در جمع‌آوری دادگان و مدل جادوگر ویکی‌پدیا استفاده شده است. به علاوه این تصمیم کمک می‌کند تا کیفیت مدل‌های انتخاب دانش و تولید پاسخ مدل حاصل از این پژوهش با مدل پایه بهتر مقایسه شوند. شایان ذکر است که این مدل بازیابی اطلاعاتی بر روی اسناد ویکی‌پدیا عمل جستجو را انجام می‌دهد و در نتیجه منبع دانش گپ‌زن ما ویکی پدیا است.

ساختار کلی پیشنهاد شده دارای دو مزیت است. مزیت اول این است که این مدل
به آسانی و با تعویض منبع دانش قابل سوارشدن بر روی سایر منابع دانش متنی است. مزیت دوم نیز است که، به علت جدا بودن زیر مسائل از هم، می‌توان در هر بخش راهکار منحصر به فردی را که مخصوص آن بخش است پیشنهاد داد و عملکرد هر بخش را جداگانه بهینه کرد. برای مثال می‌توان در بخش انتخاب دانش از مدل‌های از پیش آموزش داده‌ای که بازنمایی بهتری از متن ارائه میدهند استفاده کرد و در بخش تولید پاسخ نیز از مدل های از پیش آموزش داده شده‌ای که در وظیفه تولید متن قوی‌ترند بهره برد.

این طراحی غیر انتها به انتها و قطعه قطعه اما خود از عیب جانبی رنج می‌برد و آن نیز احتمال انتقال خطا از گامی به گام دیگر است. برای مثال در صورتی که مدل انتخاب دانش به اشتباه جمله‌ای را برگزیند، در گام بعدی نیز مدل تولید پاسخ با ورودی غلطی مواجه شده و خروجی غلطی را تولید می‌کند. از این رو پیمانه‌ای بودن این مدل و غیر انتها به انتها آموزش دیدن آن را می‌توان یکی از چالش‌های اساسی آن برشمرد.
\section{روش پیشنهادی برای بخش انتخاب دانش}

\subsection{پیشنهادات} \label{chap4:solution:recom}

در روش پیشنهادی مدل پایه برای زیرمساله انتخاب دانش، تاریخچه گفت و گو و جملات نامزد دانش همگی با یک ترنسفورمر یکسان و به طور مستقل از یکدگیر کد می‌شوند. سپس مشابهت هر یک از جملات نامزد دانش با تاریخچه، از طریق محاسبه ضرب داخلی بردار کد جمله نامزد با بردار کد تاریخچه حساب می‌‌شود و در نهایت
جمله نامزد با بیشترین میزان مشابهت با تاریخچه، به عنوان جمله دانش انتخاب می‌گردد. در هنگام آموزش مدل نیز،‌ این شبکه از طریق کاهش انتروپی متقاطع میزان شباهت جملات نامزد دانش با تاریخچه، آموزش می‌یابد. با توجه به تابع هزینه استفاده شده در این مدل، می‌توان آن را به نوعی مشابه مسئله دسته‌بندی چند دسته‌ای در نظر گرفت. 

دو اشکال بزرگ را بر مدل مذکور می‌توان وارد دانست. اولین اشکال در نحوه کد‌شدن مستقل هر یک از جملات نامزد نسبت به تاریخچه گفت و گو است. در واقع این طرح که هر یک از جملات نامزد و تاریخچه به صورت مستقل کد شوند و در نهایت برداز بازنمایی حاصل از آن‌ها با یکدیگر مشابهت سنجی شود طرح معیوبی است؛ چه آن که در هنگام کدشدن ممکن است ويژگی‌های متمایز جملات نامزد حذف شده یا بار اطلاعاتی آن‌ها در بردار کد کاهش یافته باشد و از آن‌جایی که اکثریت آن‌ها با همدیگر از یک سند اطلاعاتی استخراج شده‌اند، در نتیجه ممکن است که بردار کد همگی‌ آن‌ها در یک محدوده کوچک واقع شود. در این حال برای یافتن میزان مشابهت میان بردار کد تاریخچه و هر یک از جملات نامزد نمی‌توانیم تنها به معیار ساده ضرب داخلی تکیه کنیم. به علاوه، در بسیاری از حالت توجه توام و همزمان مدل به تاریخچه و جمله دانش در راه پیدا کردن میزان مشابهت آن‌ها می‌تواند موثرتر از توجه مستقل شبکه به هر یک از آن‌ها باشد. 

پیشنهاد اساسی ما این است که به جای آموزش دادن مدلی جهت یادگیری ارائه بازنمایی از جملات و سپس محاسبه میزان شباهت بردار‌های بازنمایی آن‌ها، یک مدل را جهت محاسبه میزان شباهت دو دنباله متنی آموزش دهیم. در واقع این مدل پیشنهادی در ورودی خود دو دنباله را می‌گیرد (تاریخچه و جمله نامزد) و در خروجی خود میزان شباهت این دو دنباله متنی را برمی‌گرداند. نقطه قوت این مدل می‌تواند در توجه توام و همزمان دنباله تاریخچه و جمله نامزد نسبت به یکدیگر باشد. در صورت استفاده از این مدل، برای یافتن جمه دانش، کافی است تا میزان مشابهت هر یک از جملات نامزد نسبت به دنباله تاریخچه محاسبه شده و سپس جمله‌ نامزد با بیشترین مشابهت به عنوان جمله دانش انتخاب شود. 

دومین اشکال وارد بر مدل انتخاب دانش جادوگر ویکی‌پیدا را می‌توان در طرز آموزش آن دانست. استفاده از تابع انتروپی متفاطع بین میزان شباهت جمله‌های نامزد با تاریخچه در هنگام آموزش، مدل را متمایل می‌کند تا صرفا به بالابردن امتیاز جمله درست توجه کند. تابع انتروپی متقاطع در مسائل دسته‌بندی که هر نمونه تنها می‌تواند متعلق به یک کلاس باشد موثر واقع می‌شود؛ اما در زیرمساله انتخاب دانش که ذات مساله بیشتر بر مسائل
\trans{رتبه‌بندی}{Ranking}
تکیه دارد، می‌تواند باعث کج‌فهمی مدل شود. مدل بایستی به جای آموزش بر این گمانه که میزان شباهت تاریخچه با جمله درست دانش نسبت به میزان شباهت تاریخچه با هر یک جملات غیردرست بایستی مانند نسبت به صفر به یک باشد؛ بر این اصل آموزش ببیند که میزان شباهت تاریخچه با جمله درست از میزان شباهت تاریخچه با جمله غیردرست بیشتر است. 

بنابراین پیشنهاد دوم ما بر این است که به جای آن که مدل در هنگام آموزش هر بار امتیاز مشابهت جمله درست دانش و همه جملات نادرست دانش را همزمان در نظر بگیرد، هر بار به مقایسه میزان مشابهت یک جفت جمله دانش درست و جمله دانش نادرست بپردازد. به فرض مثال اگر برای یک تاریخچه چهل جمله از منبع دانش استخراج شده باشد که یکی از آن‌ها درست باشد و باقی جملات نادرست باشند، در روش جادوگر ویکی پدیا امتیاز مشابهت این چهل جمله همزمان با یکدگیر مقایسه می‌شود، در حالی که در روش پیشنهاد شده، جمله دانش درست بایستی در سی و نه رقابت با سی و نه جمله نادرست یک به یک پیروز شود. ترکیب این پیشنهاد 
و پیشنهاد قبل، می‌تواند باعث خلق مدلی شود که در زمینه مساله رتبه‌بندی 
و یا برای مثال پیشنهاد پنج جمله دانش به جای یک جمله دانش
برتر از مدل جادوگر ویکی‌پدیا عمل کند.

پیشنهاد سوم ما استفاده از مدل‌های برت مینیاتوری در طراحی معماری مدل زیرمساله انتخاب دانش است
\cite{turc2019well}.
این مدل‌ها از طرفی حجم‌شان آن قدر بزرگ نیست که نتوان آن‌ها را بر روی سخت‌افزار‌های هرچند محدود در دسترس آموزش داد و برای وظیفه موردنظر تنظیم کرد و از طرف دیگر، قدرت آ‌ن‌ها و عصاره دانش قرار گرفته در پارامتر‌های این شبکه می‌تواند نقطه آغاز خوبی برای آموزش شبکه هدف باشد. به علاوه در صورتی که ما یک معماری را از نو و بدون پیش آموزش بر روی دادگان خود آموزش دهیم، این بیم می‌رود که مدل حاصل در نهایت بر روی موضوعات دیده شده در دادگان بیش‌برازش کرده و نتواند قدرت تعمیم دهی به سایر موضوعات را داشته باشد. 

\subsection{معماری پیشنهادی}\label{chap4:knowledge_selction:arch}

مدل پیشنهادی ارائه شده در این پژوهش در واقع  متکی بر یک شبکه عصاره‌‌گرفته شده از برت است (که خود می‌تواند در تنظیمات مختلف تعداد لایه و اندازه حالت نهان باشد). 
ورودی‌های این شبکه تاریخچه گفت و گو و جمله دانشی هستند که قرار است میزان شباهت آن‌ها توسط شبکه خروجی داده شود. تاریخچه گفتگو، خود از متن نوبت‌های مختلف گفتگو میان گپ‌زن و عامل انسانی پیشرو او تشکیل شده اند. این نوبت‌ها به ترتیب به یکدیگر از سمت راست الحاق شده و بین هر دو نوبت نیز توکن 
$[SEP]$
به منظور نشان دادن به پایان رسیدن نویت جای می‌گیرد. سپس جمله دانش مورد نظر نیز از سمت راست به تاریخچه گفت و گو الحاق می‌شود و مانند قبل، بین تاریخچه گفت و گو و جمله دانش نیز یک توکن 
$[SEP]$
قرار می‌گیرد. حال با یک دنباله متنی مواجه هستیم که مرز میان نوبت‌های مختلف گفتگو و جمله دانش در آن با 
$[SEP]$
از یکدیگر جدا شده است. سپس توکن‌های 
$[CLS]$
و 
$[SEP]$
به ترتیب به ابتدا و انتهای این دنباله متنی اضافه می‌شوند. 

دنباله متنی حاصل به شبکه عصاره‌گرفته‌شده برت 
با اندازه حالت نهان 
$d_{model}$
ورودی داده می‌شود.
ضمنا تعبیه قطعه‌ای شبکه برت برای زیردنباله تاریخچه مقدار صفر و برای زیردنباله جمله دانش نیز مقدار یک تنظیم می‌شوند و به شبکه ورودی داده می‌شوند. 
 سپس بردار بازنمایی کدشده متعلق به توکن
$[CLS] $
که به نوعی بازنمایی تمام دنباله است،‌ به وسیله 
یک لایه پیشرو تمام متصل با اندازه
$d_{model}$
به 
$1$،
 به عدد 
 $o$
  تبدیل می‌شود. حال این عدد $o$ نیز طبق رابطه 
  \ref{eq:similarity_equation}
   به میزان مشابهت که $similarity$ نام دارد تبدیل می‌شود. 
  
  \begin{gather}\label{eq:similarity_equation}
  similarity = \frac{\tanh{(o)}-(-1)}{2}
  \end{gather}

رابطه 
\ref{eq:similarity_equation}
کمک می‌کند تا میزان مشابهت خروجی از شبکه همواره عددی بین صفر و یک باشد. 

\subsection{توابع هزینه}
در بخش قبل ساختار پیشنهادی معماری شبکه‌ای که قادر به محاسبه میزان شباهت تاریخچه و جمله دانش باشد ارائه شد. حال بایستی نحوه آموزش و بهینه‌کردن شبکه مطرح شود. به صورت کلی همان‌طور که در قسمت پیشنهادات ابتدای فصل مطرح شد، بایستی هر بار میزان مشابهت تاریخچه با جمله درست دانش و میزان مشابهت تاریخچه با جمله نادرست دانش با یکدیگر سنجیده شده و مقدار عبارت اول بیشتر از دومی باشد. ما در ادامه عبارت اول را 
$true\:similarity$
و عبارت دوم را نیز
$false\:similarity$
نامگذاری می‌کنیم. در ادامه دو تابع هزینه جهت نیل به هدف مطرح شده ارائه شده اند:

\begin{itemize}
	\item 
	\textbf{
	تابع خطای انتروپی متقاطع 
	\trans{دوتایی}{binary}
	:
	}

	رابطه این تابع که از ایده مدل رگرسیون منطقی برآمده است در رابطه
	\ref{eq:log_loss_function}
	آمده است. 
\begin{gather}\label{eq:log_loss_function}
\mathcal{L}_{log} = -\log{(true\:similarity)} -  \log{(1-false\:similarity)} 
\end{gather}


	\item
	\textbf{
	تابع خطای خطی:
	}
رابطه این تابع که ساده‌تر از تابع قبل است، در رابطه 
\ref{eq:linear_loss_function}
آمده است.
\begin{gather}\label{eq:linear_loss_function}
\mathcal{L}_{lin} = -true\:similarity + false\:similarity 
\end{gather}


\end{itemize}

شبکه سپس با یکی از این دو تابع خطای ذکر شده آموزش یابد و سعی در کمینه کردن مقدار آن‌ها کند. در فصل آتی، آموزش یافتن شبکه با هر یک از این دو تابع خطا مورد آزمایش قرار گرفته و نتایج ارائه خواهند شد.

\section{روش پیشنهادی برای بخش تولید پاسخ}

پایه و اساس معماری پیشنهادی برای این زیرمساله، معماری دنباله به دنباله است. با توجه به قدرت و توانایی مدل‌های از پیش آموزش داده شده، در این قسمت نیز از این مدل‌ها استفاده خواهد شد.
شبکه‌ای که در این قسمت بایستی تعریف شود،‌ در ورودی خود تاریخچه گفتگو و جمله دانش را می‌گیرد و در قسمت خروجی خود پاسخ را تولید می‌کند. 
\subsection{معماری پیشنهادی}

\subsubsection{معماری برت به برت} \label{chap4:generation:bert2bert}

معماری‌های کدگذار نظیر برت، به علت پیش آموزشی که از قبل روی دادگان عظیم متنی داشته‌اند قادر به ارائه بازنمایی مناسبی از دادگان متنی هستند. این بازنمایی در واقع حاصل مقدار پارامتر‌های شبکه برت است. می‌توان انتظار داشت که در صورتی که برای هر وظیفه متنی،‌ شبکه برت و پارامتر‌های آن به عنوان نقطه شروع اولیه در نظر گرفته شوند و سپس بنابر آن وظیفه، پارامتر‌های شبکه مورد تنظیم و به روز رسانی واقع شوند نتیجه بهتری حاصل خواهد شد تا آن که شبکه‌ای از نو طراحی شده و پارامتر‌های آن نیز به صورت کاملا تصادفی و فاقد دانش بخواهند مقدار دهی اولیه شوند. 

معماری دنباله به دنباله ترنسفورمری از دو قسمت کدگذار و کدگشا تشکیل شده است. قسمت کدگذار را می‌توان به راحتی با یک برت مینیاتوری جایگزین نمود. اما در مورد قسمت کدگشا، نمی‌توان به جای آن  یک معماری مبتنی بر برت را قرار داد؛ چرا که قسمت کدگشا دارای لایه‌های 
\trans{توجه متقاطع}{Cross Attention}
هستند. از طرفی مکانیزم توجه در سمت کدگشا به گونه‌ای است که هر توکن صرفا به توکن‌های قبل از خود می‌تواند توجه کند، حال آن که در شبکه برت هر توکن می‌تواند هم به توکن‌های سمت راست و هم به توکن‌های سمت چپ خود توجه کند. 

پس به منظور استفاده از برت در جایگاه کدگشا بایستی در آن اصلاحاتی صورت بگیرد. در ابتدا، به مانند معماری ترنسفورمری،‌ بلوک‌های توجه متقاطع در لایه‌های برت اضافه می‌شوند و پارامتر‌های آن‌ها نیز به صورت تصادفی مقداردهی اولیه می‌شوند. همچنین نحوه توجه در شبکه برت کدگشا نیز به گونه‌ای تنظیم می‌شود که هر توکن صرفا به توکن قبل از خود بتواند توجه کند.

حال دو برت کدگذار و کدگشا به یکدیگر متصل شده و یک شبکه دنباله به دنباله را تشکیل می‌دهند. تاریخچه گفت و گو و جمله دانش به مانند تنظیماتی که در 
بخش 
\ref{chap4:knowledge_selction:arch}
گفته شد، با یکدیگر یک دنباله را تشکیل داده و به عنوان ورودی به مدل برت به برت داده می‌شوند. در قسمت کدگشا نیز به ابتدا و انتهای پاسخ به ترتیب توکن‌های
$[CLS]$
و
$[SEP]$
اضافه می‌شوند و مدل با کمینه‌کردن منفی لگاریتم درست‌نمایی پاسخ‌های تولید شده اش  آموزش می‌یابد.  

\subsubsection{معماری مبتنی بر بارت} \label{chap4:generation:bart}

بارت همان‌گونه که در فصل قبل بیان شد، ذاتا یک مدل دنباله به دنباله از پیش آموزش یافته است. از بارت اغلب در مسیر حل مساله خلاصه‌سازی (که خود یک مساله دنباله به دنباله است) استفاده شود. در این پژوهش پیشنهاد می‌شود تا از بارت در زیرمساله تولید پاسخ نیز استفاده شود. 

در ابتدا نوبت‌های مختلف تاریخچه گفتگو از سمت راست به یکدیگر الحاق می‌شوند و در پایان هر نوبت نیز توکن
$</s>$
جای می‌گیرد. سپس در ابتدای نوبت اول نیز توکن
$<s>$
اضافه می‌شود. حال بایستی جمله دانش نیز به این دنباله فعلی اضافه شود. برای تمایز جمله دانش از محتوای گفتگو، توکن‌های 
" \lr{`} "
و
" \lr{\textasciitilde} "
به ترتیب به ابتدا و انتهای جمله دانش اضافه می‌شوند و دنباله حاصل به دنباله تاریخچه گفت و گو از سمت راست الحاق می‌شود. حال دنباله نهایی که شامل تاریخچه و جمله دانش است به عنوان ورودی به قسمت کدگذار بارت داده می‌شود. 

در قسمت کدگشا، در هنگام آموزش، به ابتدای و انتهای پاسخ‌ها به ترتیب توکن‌های 
$<s>$
و
$</s>$
اضافه می‌شوند و مدل بایستی با توجه به خروجی قسمت کدگذار، آموزش دیده شود تا مقدار منفی لگاریتم درست‌نمایی پاسخ‌ تولید شده‌اش با عنایت به پاسخ درست کمینه شود.

\section{جمع‌بندی}
در این فصل راهکار پیشنهادی برای دانش‌بنیان کردن گپ‌زن مطرح شد. این راهکار پیشنهادی خود از حل سه زیر مساله تشکیل شده است که توجه این پژوهش بر حل زیرمساله‌های دوم و سوم متمرکز شده است. برای حل زیر مساله دوم یا انتخاب دانش، تغییر رویکرد مساله به رتبه‌بندی دوتایی پیشنهاد شد. ابتکار قابل توجه در این بخش را می‌توان تغییر هدف شبکه مورد آموزش این زیرمساله از به دست آوردن بازنمایی عبارات به یادگیری محاسبه میزان شباهت آن‌ها (در حالی که به هر دو توجه توامان دارد) دانست. در ادامه دو تابع هزینه برای آموزش شبکه برای بخش انتخاب دانش پیشنهاد شدند که نتایج عملکرد آن‌ها در فصل بعدی سنجیده خواهد شد.

پس از ارائه راهکار برای انتخاب دانش، برای زیرمساله تولید پاسخ نیز دو دسته معماری پیشنهاد شد. دسته‌ معماری اول بر استفاده از برت به عنوان کدگذار و کدگشا و دسته معماری دوم نیز بر استفاده از شبکه دنباله به دنباله بارت تکیه دارند. عملکرد این دو دسته شبکه نیز در فصل بعدی مقایسه خواهد شد. 


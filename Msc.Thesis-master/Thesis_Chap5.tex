\chapter{آزمایش و نتایج}\label{Chap5}
\minitoc

پس از تقسیم‌بندی مسئله به زیرمسائل مختلف و ارائه دادن مدل‌های پیشنهادی برای هر یک از زیر مسائل در فصل پیش، در این فصل نتایج پیاده‌سازی و آموزش مدل‌ها ارائه خواهد شد و در مورد مقایسه آن‌ها با یکدیگر و نتایجشان نیز بحث صورت خواهد گرفت. در ضمن از آن‌جایی که مدل پیشنهاد شده برای گپ‌زن، یک مدل چند بخشی است، ارزیابی گپ‌زن در سه بخش مجزا انجام خواهد شد. در واقع در دو زیر مساله انتخاب دانش و تولید پاسخ ارزیابی گپ‌زن به صورت جزیی و در بخش سوم نیز ارزیابی کلی سامانه گپ‌زن (با توجه به احتمال انتشار خطا از مرحله‌ای به مراحل بعدی) صورت خواهد پذیرفت.

\section{محیط آزمایش و ابزارها}

پیاده‌سازی مدل‌های این پژوهش و آموزش‌‌ ‌آن‌ها با استفاده از 
\trans{چارچوب}{Framework}
\trans{پایتورچ}{Pytorch}
که مبتنی بر زبان
\trans{پایتون}{Python}
است صورت گرفته است. همچنان جهت بارگیری‌کردن و استفاده از مدل‌های از پیش آموزش دیده شده نیز از کتابخانه ارزشمند 
\lr{HuggingFace}
بهره برده شده است. 

آموزش و تنظیم‌ مدل‌های این پژوهش نیز همگی با استفاده از سرویس 
\lr{Google Colab}
انجام گرفته‌اند. از آنجایی که حجم دادگان این پژوهش بعضا بسیار فراتر از آن است که بتوان در چند ساعت بر روی آن‌ها یکبار عمل آموزش شبکه را انجام داد و هم این که سرویس 
\lr{Colab}
پس از هر ۹ ساعت عملیات قبلی را متوقف می‌کند،‌ ترتیبی در پیاده‌سازی مکانیزم آموزش مدل‌ها اندیشیده شده است که مشخصات محیط آزمایش و دادگانی که مورد آموزش واقع ‌شده‌اند حفظ و بتوانند بازیابی شوند تا همه دادگان سهم یکسانی در آموزش مدل داشته باشند. 



\section{ارزیابی مدل انتخاب دانش}
\subsection{مقدمه}
در این بخش توضیحات و نتایج مربوط به پیاده‌سازی مدل‌های پیشنهاد شده انتخاب دانش آورده شده‌اند. قبل از ذکر این نتایج لازم است تا توضیحی در مورد نحوه آموزش مدل‌ها و پیش‌پردازش‌های انجام داده و بهینه‌سازهای استفاده شده، آورده شود.

 ابتدا جهت آموزش مدل انتخاب دانش،‌ جفت دنباله‌هایی که به منظور محاسبه 
 \lr{true similarity}
 و
 \lr{false similarity}
 در بخش 
 \ref{chap4:solution:recom}
 مطرح شدند، بایستی استخراج شوند. بدین منظور تمامی جملات دانشی که بخش بازیابی اطلاعات در هنگام جمع آوری دادگان به عامل انسانی پیشنهاد داده و همچنین جمله دانشی که عامل انسانی از آن استفاده کرده جمع آوری می‌شوند. در صورتی که عامل انسانی از جمله‌ای استفاده نکرده باشد،‌ جمله دانش آن را رشته خالی در نظر می‌گیریم. حال به ازای هر نوبت از مکالمه زوج‌های جمله دانش درست و جمله دانش نامزد ، گردآوری می‌شوند. در نهایت محصول این مراحل 2775678 نمونه هستند که هر نمونه شامل دو زوج (تاریخچه گفتگو و جمله دانش صحیح) و (تاریخچه گفتگو و جمله دانش ناصحیح) است. 
 

از آن‌جایی که اولا مدل برت تنها می‌تواند دنباله‌هایی با طول حداکثر ۵۱۲ توکن را پردازش کند و ثانیا در صورت استفاده از دنباله‌های بلند در فرآیند یادگیری، با مشکل کمبود حافظه در سخت افزار مواجه می‌شویم، لذا لازم است تا مکانیزمی جهت کوتاه کردن دنباله ورودی به شبکه برت طراحی و اعمال شود. در این مکانیزم حداکثر طول دنباله ورودی به شبکه برت ۱۲۸ توکن فرض می‌شود و در صورتی که مجموع طول دنباله‌های تاریخچه گفتگو و جمله دانش بیشتر از این مقدار باشند، بایستی این دنباله‌ها کوتاه شوند. به این منظور دو دنباله تاریخچه و جمله دانش در نظر گرفته می‌شوند و هر بار از بلندترین آن‌ها یک توکن بریده می‌شود. اعمال بریدن توکن بر روی دنباله تاریخچه از سمت چپ و بر روی جمله دانش از سمت راست اعمال می‌شود. در نهایت حاصل از این مرحله دنباله‌های ورودی است که حداکثر ۱۲۸ توکن دارند. 

سپس دادگان تحت عمل دسته‌بندی به دسته‌های ۱۲۸تایی تقسیم می‌شوند و به شبکه به عنوان ورودی داده می‌شوند. برای بهینه‌سازی شبکه نیز از بهینه‌ساز آدام
\footnote{Adam}
با ترخ یادگیری ثابت 
$2e^{-5}$
استفاده می‌شود.

برای ارزیابی مدل‌های این بخش از دو معیار امتیاز
\lr{f1}
بین جمله انتخاب شده و جمله صحیح
و نرخ‌های 
\lr{Recall@1} و \lr{Recall@3} و \lr{Recall@5}
استفاده می‌شود. امتیاز
\lr{Recall@i}
در واقع نشان می‌دهد که در صورتی که مدل هر بار 
\lr{i}
جمله  با بیشترین امتیاز تخمین زده توسط خود را به عنوان نامزد‌های جمله دانش برگرداند، در چند درصد این موارد، جمله دانش صحیح در بین این 
\lr{i}
جمله خروجی قرار دارد. 
به علاوه معیاری تحت عنوان نرخ باخت دوتایی درست نیز ارائه خواهند که نشان می‌دهند مدل در چند مورد از نمونه‌های جفت جمله دانش صحیح و جمله دانش ناصحیح نتوانسته به جمله دانش صحیح امتیاز بالاتری دهد. 
از طرف از آن‌ جایی که دادگان تست جادوگر ویکی پدیا در دو قالب دادگان تست با موضوع آشنا و دادگان تست با موضوع ناآشنا عرضه شده‌اند، امتیاز مدل‌ها بر روی هر دوی این دادگان سنجیده می‌شود تا قدرت تعمیم‌دهی آن‌ها نسبت به موضوعات متفاوت نیز ارزیابی شود.  

هر یک از مدل‌ها یک 
\trans{دور}{Epoch}
معادل با بیش از بیست هزار گام روی دادگان آموزشی، آموزش می‌یابند.

\subsection{ارزیابی مدل انتخاب دانش مبتنی بر تابع هزینه‌ی لگاریتمی }
در این بخش مدل‌ پیشنهاد شده با تابع هزینه ارائه شده در بخش 
\ref{eq:log_loss_function}
آموزش یافته و نتایج عملکرد آن استخراج شده است. نتایج عملکرد این مدل‌ها بر روی دادگان تست با موضوع آشنا و موضوع ناآشنا در جدول‌های 
\ref{table:knowledge:log:unseen}
و 
\ref{table:knowledge:log:seen}
آمده است. تنظیمات و معماری این مدل‌ها نیز (تعداد لایه و اندازه حالت نهان) در ستون نام مدل آورده شده است.
\footnote{برای مثال مدل دو لایه با اندازه نهان ۱۲۸، به مدلی اشاره می‌کند که نقطه شروع اولیه آن برت مینیاتوری با ۲ لایه و اندازه نهان ۱۲۸ است.}

\begin{table}[h]
	\caption{نتایج بر روی دادگان آشنا مدل‌های انتخاب دانش با آموزش روی تابع خطای لگاریتمی }
	\centering
	\label{table:knowledge:log:seen}
	\begin{tabular}{|l|l|l|l|l|l|}
		\hline
		نام مدل                      & نرخ باخت & R@1     & R@3     & R@5     & f1      \\ \hline
		مدل پایه جادوگر ویکی‌پدیا    & -        & $24.5$  & -       & -       & $36.4$  \\ \hline
		دو لایه با اندازه نهان ۱۲۸   & $9.26$   & $34.97$ & $57.35$ & $34.97$ & $40.07$ \\ \hline
		چهار لایه با اندازه نهان ۲۵۶ & $8.33$   & $45.73$ & $67.84$ & $79.01$ & $53.26$ \\ \hline
		چهار لایه با اندازه نهان ۵۱۲ & $8.62$   & $48.98$ & $70.76$ & $81.40$ & $55.15$ \\ \hline
		شش لایه با اندازه نهان ۲۵۶   & $7.60$   & $48.14$ & $70.06$ & $80.51$ & $54.29$ \\ \hline
	\end{tabular}
\end{table}

\begin{table}[h]
	\caption{نتایج بر روی دادگان ناآشنا مدل‌های انتخاب دانش با آموزش روی تابع خطای لگاریتمی }
	\centering
	\label{table:knowledge:log:unseen}
	\begin{tabular}{|l|l|l|l|l|l|}
		\hline
		نام مدل                      & نرخ باخت & R@1     & R@3     & R@5     & f1      \\ \hline
		مدل پایه جادوگر ویکی‌پدیا    & -        & $23.7$  & -       & -       & $35.8$  \\ \hline
		دو لایه با اندازه نهان ۱۲۸   & $16.19$  & $22.64$ & $40.43$ & $54.38$ & $38.60$ \\ \hline
		چهار لایه با اندازه نهان ۲۵۶ & $11.76$  & $34.92$ & $54.66$ & $68.63$ & $44.39$ \\ \hline
		چهار لایه با اندازه نهان ۵۱۲ & $11.87$  & $37.30$ & $59.00$ & $70.76$ & $47.20$ \\ \hline
		شش لایه با اندازه نهان ۲۵۶   & $11.75$  & $35.35$ & $55.90$ & $68.58$ & $45.94$ \\ \hline
	\end{tabular}
\end{table}


\subsection{ارزیابی مدل انتخاب دانش مبتنی بر تابع هزینه خطی}

در این بخش مدل‌ پیشنهاد شده با تابع هزینه ارائه شده در بخش 
\ref{eq:linear_loss_function}
آموزش یافته و نتایج عملکرد آن استخراج شده است. نتایج عملکرد این مدل‌ها بر روی دادگان تست با موضوع آشنا و موضوع ناآشنا در جدول‌های 
\ref{table:knowledge:lin:unseen}
و 
\ref{table:knowledge:lin:seen}
آمده است.

\begin{table}[h]
	\caption{نتایج بر روی دادگان آشنا مدل‌های انتخاب دانش با آموزش روی تابع خطای خطی }
	\centering
	\label{table:knowledge:lin:seen}
	\begin{tabular}{|l|l|l|l|l|l|}
		\hline
		نام مدل                      & نرخ برد & R@1     & R@3     & R@5     & f1      \\ \hline
		مدل پایه جادوگر ویکی‌پدیا    & -       & $24.5$  & -       & -       & $36.4$  \\ \hline
		دو لایه با اندازه نهان ۱۲۸   & $5.65$  & $84.84$ & $86.37$ & $88.30$ & $75.20$ \\ \hline
		چهار لایه با اندازه نهان ۲۵۶ & $3.62$  & $79.32$ & $83.88$ & $88.88$ & $74.22$ \\ \hline
		چهار لایه با اندازه نهان ۵۱۲ & $3.07$  & $83.71$ & $87.13$ & $90.72$ & $77.93$ \\ \hline
		شش لایه با اندازه نهان ۲۵۶   & $3.29$  & $84.58$ & $86.92$ & $89.57$ & $76.88$ \\ \hline
	\end{tabular}
\end{table}

\begin{table}[h]
	\caption{نتایج بر روی دادگان ناآشنا مدل‌های انتخاب دانش با آموزش روی تابع خطای خطی }
	\centering
	\label{table:knowledge:lin:unseen}
	\begin{tabular}{|l|l|l|l|l|l|}
		\hline
		نام مدل                      & نرخ برد & R@1     & R@3     & R@5     & f1      \\ \hline
		مدل پایه جادوگر ویکی‌پدیا    & -       & $23.7$  & -       & -       & $35.8$  \\ \hline
		دو لایه با اندازه نهان ۱۲۸   & $8.72$  & $76.52$ & $79.12$ & $81.80$ & $66.58$ \\ \hline
		چهار لایه با اندازه نهان ۲۵۶ & $5.25$  & $70.02$ & $76.19$ & $83.21$ & $66.34$ \\ \hline
		چهار لایه با اندازه نهان ۵۱۲ & $4.34$  & $74.02$ & $79.10$ & $84.81$ & $69.58$ \\ \hline
		شش لایه با اندازه نهان ۲۵۶   & $5.15$  & $75.48$ & $78.99$ & $83.95$ & $71.05$ \\ \hline
	\end{tabular}
\end{table}


\section{ارزیابی مدل تولید پاسخ}

\subsection{مقدمه}
در این بخش مدل‌های پیشنهاد شده در بخش‌های 
\ref{chap4:generation:bert2bert}
و
\ref{chap4:generation:bart}
پیاده‌سازی و نتایج عملکرد آن‌ها گزارش شده است. از آنجایی که هنگام تولید پاسخ، استراتژی‌ها و رویکرد‌های متفاوتی را می‌توان در پیش گرفت (نظیر اتخاذ اندازه پرتو یا حریصانه یا بهینه اعمال کردن در هنگام تولید متن) ؛ تاثیر اتخاذ این سیاست‌های مختلف نیز در مورد هر مدل بررسی شده است.

عملکرد هر یک از مدل‌ها در تولید پاسخ، با استفاده از معیار‌های امتیاز مشابهت
\lr{F1}
با پاسخ صحیح، میزان سرگشتگی مدل در هنگام تولید پاسخ صحیح و معیار
\lr{BertScore}
پاسخ تولید شده، سنجیده شده است.

دادگان آموزشی اصلی مورد استفاده در این بخش دادگان  مربوط به پاسخ‌های تولیدشده توسط نقش معلم در جادوگر ویکی پدیا هستند. این دادگان در مجموع ۴۱۴۸۹ نمونه هستند. هر یک از این نمونه‌ها دارای سه جز اساسی تاریخچه گفتگو، جمله دانش و پاسخ است که دنباله متشکل از تاریخچه گفتگو و جمله دانش برای هر یک از نمونه‌ها به وسیله استراتژی مشابه با مورد بخش
\ref{chap4:solution:recom}
به دنباله‌هایی با حداکثر طول ۱۲۸ توکن محدود می‌شوند. این نمونه‌های سپس به ۲۵۹۴ دسته شانزده تایی تقسیم می‌شوند. با این حال در زمان آموزش با استفاده از شگرد 
\trans{تجمیع گرادیان}{gradient accumulation}
، هر چهار گام یکبار عمل به روزرسانی و بهینه‌سازی روی شبکه صورت خواهد گرفت (به عبارت بهتر پس از مشاهده هر چهار دسته ۱۶ تایی شبکه مورد آموزش توسط بهینه‌ساز واقع خواهد شد). بهینه‌ساز مورد استفاده در این بخش نیز مانند مسئله قبلی بهینه‌ساز آدام است که نرخ یادگیری آن نرخ ثابت 
$1e-5$
است. 

نکته لازم به ذکر در مورد ارزیابی در این بخش این است که، در موقع ارزیابی جملات دانش صحیح به عنوان ورودی به مدل داده‌ می‌شوند و در واقع توانایی تولید پاسخ مدل با توجه با فرض ورودی گرفتن جمله دانش صحیح سنجیده می‌شود. 

\subsection{مدل برت به برت}
این مدل که در بخش 
\ref{chap4:generation:bert2bert}
مورد پیشنهاد و بررسی قرار گرفت در چهار تنظیمات مورد پیاده‌سازی و آموزش قرار گرفته است. این چهار تنظیمات، دو لایه با اندازه ۱۲۸، چهارلایه با اندازه نهان ۲۵۶، شش لایه با اندازه نهان ۵۱۲ و هشت لایه با اندازه نهان ۷۶۸ هستند. برای مثال مدل با تنظیمات چهارلایه و اندازه نهان ۲۵۶ از دو رمزگذار و رمزگشا تشکیل شده است که هر کدام چهارلایه با اندازه نهان ۲۵۶ دارند. 

هر یک از مدل‌های مذکور پنجاه هزار گام (معادل حدود بیست دور) روی دادگان آموزشی ویکی‌پدیا آموزش دیده‌اند و در حین آموزش هر دو هزار و پانصد گام، مدل آموزش دیده به عنوان 
\trans{نقطه بازرسی}{Checkpoint}
ذخیره شده است. در نهایت  در هر تنظمیات  هرنقطه‌ بازرسی که دارای کمترین مقدار منفی لگاریتم درست‌نمایی روی دادگان اعتبارسنجی بوده است به عنوان مدل برگزیده آن تنظیمات انتخاب شده است. سپس برای تولید متن از مدل، از استراتژی
جستجوی حریصانه (اندازه پرتو برابر با یک) استفاده شده است.  در ضمن در هنگام تولید متن نیز، مکانیزمی پیاده‌سازی شده است تا مانع بیش از یک‌بار رخداد یک سه‌تایی در متن خروجی شود. 
نتایح عملکرد این مدل‌های برگزیده روی دادگان تست در جدول فلان آمده است.
\begin{table}[h]
	\caption{جدول فلان}
	\label{table:generation:bert2bert:beam1}
	\begin{tabular}{|l|l|l|l|l|l|l|}
		\hline
		& \multicolumn{3}{c|}{عملکرد روی دادگان آشنا} & \multicolumn{3}{c|}{عملکرد روی دادگان نا‌آشنا} \\ \hline
		نام مدل        & سرگشتگی     & امتیاز F1     & BERTScore     & سرگشتگی      & امتیاز F1      & BERTScore      \\ \hline
		مدل جادوگر     & $23.1$      & $35.5$        & -             & $32.8$       & $32.2$         & -              \\ \hline
		دولایه - ۱۲۸   & $55.14$     & $24.90$       & $45.89$       & $74.44$      & $23.97$        & $44.39$        \\ \hline
		چهارلایه - ۲۵۶ & $36.96$     & $29.54$       & $48.72$       & $46.52$      & $27.95$        & $46.96$        \\ \hline
		شش لایه- ۵۱۲   & $24.04$     & $34.17$       & $52.72$       & $31.18$      & $31.83$        & $50.88$        \\ \hline
		هشت لایه- ۷۶۸  & $21.97$     & $33.77$       & $52.63$       & $28.78$      & $31.76$        & $50.53$        \\ \hline
	\end{tabular}
\end{table}


\subsection{مدل بارت}

\subsubsection{آزمایش آموزش منحصر بارت و سنجش تاثیر اندازه پرتو} \label{chap5:bart:plain}
در این بخش،‌ مدل پیشنهادی گپ‌زنی مبتنی بر مدل بارت که در بخش 
\ref{chap4:generation:bart}
مورد معرفی قرار گرفت، پیاده‌سازی و نتایج عملکرد آن گزارش شده است. مدل پایه این مدل، مدل 
\lr{bart-base}
است که دارای ۶ لایه در بخش‌های رمزگذار و رمزگشا و همچنین اندازه حالت نهان برابر با ۷۶۸ است. این مدل بیست و پنج‌هزار گام (حدود ده دور) روی دادگان آموزشی ویکی‌پدیا تنظیم یافته است. 
در حین آموزش هر دو هزار و پانصد وضعیت شبکه به عنوان نقطه بازرسی ذخیره شده و در نهایت وضعیت شبکه‌ای که دارای کمترین منفی لگاریتم درست‌نمایی روی داده‌های اعتبارسنجی بوده است به عنوان مدل نامزد برای تولید متن انتخاب شده است. 
سپس برای تولید متن،‌ دو استراتژی کلی تولید حریصانه متن و تولید متن به وسیله نمونه‌برداری تصادفی استفاده شده است. در قسمت تولید متن حریصانه، تاثر اندازه پرتو با سنجش متن‌های تولید‌شده شبکه به ازای اندازه پرتو‌های ۱ و ۴ و ۸ و ۱۶ سنجیده شده‌اند. 



% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{table}[h]
	\caption{نتایج آزمایش بارت خالی}
	\label{table:bart_plain}
	\begin{tabular}{|c|c|l|l|l|l|l|}
		\hline
		\multicolumn{1}{|l|}{}                    & \multicolumn{3}{c|}{عملکرد روی دادگان آشنا}     & \multicolumn{3}{c|}{عملکرد روی دادگان نا‌آشنا}  \\ \hline
		نام مدل                                   & سرگشتگی                 & امتیاز F1 & BERTScore & سرگشتگی                 & امتیاز F1 & BERTScore \\ \hline
		مدل جادوگر                                & $23.1$                  & $35.5$    & -         & $32.8$                  & $32.2$    & -         \\ \hline
		حریصانه                                   & \multirow{5}{*}{$9.48$} & $39.30$   & $58.36$   & \multirow{5}{*}{$9.97$} & $38.82$   & $57.59$   \\ \cline{1-1} \cline{3-4} \cline{6-7} 
		حریصانه با پرتو ۴                         &                         & $40.52$   & $57.36$   &                         & $41.18$   & $57.17$   \\ \cline{1-1} \cline{3-4} \cline{6-7} 
		حریصانه با پرتو ۸                         &                         & $40.72$   & $57.20$   &                         & $41.34$   & $57.12$   \\ \cline{1-1} \cline{3-4} \cline{6-7} 
		حریصانه با پرتو ۱۶                        &                         & $40.82$   & $57.15$   &                         & $41.20$   & $56.80$   \\ \cline{1-1} \cline{3-4} \cline{6-7} 
		\multicolumn{1}{|l|}{نمونه‌برداری تصادفی} &                         & $32.34$   & $53.24$   &                         & $32.19$   & $52.75$   \\ \hline
	\end{tabular}
\end{table}


\subsubsection{آزمایش انجام پیش آزمایش بر روی دادگان
\lr{OpenSubtitle}
}

در این آزمایش،‌ مدل بارت، قبل از آموزش دیدن و تنظیم یافتن روی دادگان جادوگر ویکی‌‌پدیا، روی دادگان استخراج شده و به قالب مساله مکالمه‌ درآمده از دادگان 
\lr{OpenSubtitle}
پیش آموزش یافته است. این آموزش به تعداد سیصدهزار گام(معادل دو دور بر روی کل دادگان 
\lr{OpenSubtitle}
)
انجام گرفته است. سپس شبکه حاصل کاملا به مانند تنظیمات آزمایش
\ref{chap5:bart:plain}
بر روی دادگان جادوگر ویکی‌پدیا تنظیم یافته است. نتایج عملکرد این مدل نیز با دو استراتژی تولد متن حریصانه و تصادفی در دو جدول فلان و فلان آمده است.

\begin{table}[h]
	\caption{نتایج آزمایش بارت خالی}
	\label{table:bart_plain}
	\begin{tabular}{|c|c|c|c|c|c|c|}
		\hline
		نام مدل        & \multicolumn{3}{c|}{عملکرد روی دادگان آشنا} & \multicolumn{3}{c|}{عملکرد روی دادگان نا‌آشنا} \\ \hline
		& سرگشتگی     & امتیاز F1     & BERTScore     & سرگشتگی      & امتیاز F1      & BERTScore      \\ \hline
		جادوگر         & $23.1$      & $35.5$        & -             & $32.8$       & $32.2$         & -              \\ \hline
		بدون پیش‌آموزش & $9.48$      & $39.30$       & $58.36$       & $9.97$       & $38.82$        & $57.59$        \\ \hline
		با پیش آموزش   & $9.67$      & $39.23$       & $58.45$       & $10.27$      & $39.18$        & $58.24$        \\ \hline
	\end{tabular}
\end{table}

\subsubsection{آزمایش بیش‌برازش مدل}
در این آزمایش مدل بارت ابتدا ۱۵۰ هزار گام روی دادگان
\lr{OpenSubtitle}
آموزش دیده و سپس ۱۵۰ هزارگام نیز روی دادگان جادوگر ویکی‌پدیا تنظیم یافته است. در طول ۱۵۰ هزار گام دوم، مدل بر روی دادگان آموزشی جادوگر ویکی‌پدیا دچار بیش برازش شده و علی رغم کاهش مقدار تابع هزینه آن روی دادگان آموزشی، مقدار تابع هزینه آن روی دادگان اعتبارسنجی دچار افزایش شده است. هدف از انجام این آزمایش بررسی تاثیر بیش‌برازش در کیفیت متن تولیدی مدل است. نتایج کیفیت متن‌های تولیدی مدل حاصل از این آزمایش در جدول‌های فلان و فلان آورده شده است. 

\begin{table}[h]
	\caption{نتایج آزمایش بارت خالی}
	\label{table:bart_plain}
	\begin{tabular}{|c|c|c|c|c|c|c|}
		\hline
		نام مدل         & \multicolumn{3}{c|}{عملکرد روی دادگان آشنا} & \multicolumn{3}{c|}{عملکرد روی دادگان نا‌آشنا} \\ \hline
		& سرگشتگی     & امتیاز F1     & BERTScore     & سرگشتگی      & امتیاز F1      & BERTScore      \\ \hline
		جادوگر          & $23.1$      & $35.5$        & -             & $32.8$       & $32.2$         & -              \\ \hline
		مدل اصلی        & $9.48$      & $39.30$       & $58.36$       & $9.97$       & $38.82$        & $57.59$        \\ \hline
		بیش‌برازش یافته & $28.50$     & $35.26$       & $55.91$       & $31.50$      & $35.53$        & $55.88$        \\ \hline
	\end{tabular}
\end{table}

\subsection{آزمایش فریب مدل}
با وجود عملکرد مثبت مدل مبتنی بر بارت، اما این بیم وجود دارد که این مدل بدون توجه به تاریخچه گفتگو، صرفا به جمله دانش توجه کرده و با تغییر آن جمله پاسخ را تولید می‌کند. بدین منظور آزمایشی برای سنجیدن این حدس طراحی می‌شود. این آزمایش از جهاتی به ایده حذف دانش تصادفی استفاده شده در مدل جادوگر ویکی‌پدیا شباهت دارد. در آن پژوهش به مانند حذف تصادفی، در هنگام آموزش جمله دانش به کلی حذف شده و مدل بایستی صرفا با داشتن تاریخچه گفتگو اقدام به تولید پاسخ می‌کرد. ایده فریب مدل اما این است که در هنگام آموزش به مدل دو جمله دانش داده شود. یکی از این جملات جمله دانش صحیح است و دیگری جمله دانشی است که به تصادف از مجموعه دادگان انتخاب شده است. این‌گونه مدل در هنگام آموزش مجبور است تا برای تشخیص جمله دانش درست به تاریخچه گفتگو نیز دقت کند و از آن نیز تاثیر بپذیرد. در هنگام سنجش نهایی مدل اما، صرفا جمله دانش درست به مدل تحویل داده می‌شود. 

برای انجام این آزمایش از مدلی با تنظیماتی مشابه 
\ref{chap5:bart:plain}
استفاده شده است و مدل بیست و پنج هزار گام آموزش دیده و نقطه بازرسی که کمترین میزان منفی لگاریتم درست‌نمایی را داشته است به عنوان مدل مورد آزمایش تولید متن انتخاب شده است.

\begin{table}[]
	\caption{نتایج آزمایش بارت خالی}
	\label{table:bart_plain}
	\begin{tabular}{|c|c|c|c|c|c|c|}
		\hline
		& \multicolumn{3}{c|}{عملکرد روی دادگان آشنا} & \multicolumn{3}{c|}{عملکرد روی دادگان نا‌آشنا} \\ \hline
		نام مدل      & سرگشتگی     & امتیاز F1     & BERTScore     & سرگشتگی      & امتیاز F1      & BERTScore      \\ \hline
		جادوگر       & $23.1$      & $35.5$        & -             & $32.8$       & $32.2$         & -              \\ \hline
		مدل اصلی     & $9.48$      & $39.30$       & $58.36$       & $9.97$       & $38.82$        & $57.59$        \\ \hline
		مدل موردفریب & $11.47$     & $35.79$       & $58.99$       & $12.46$      & $35.69$        & $55.49$        \\ \hline
	\end{tabular}
\end{table}


\section{ارزیابی کلی گپ‌زن}
در این بخش عملکرد کل سیستم گپ‌زن که شامل بخش‌های انتخاب دانش و تولید پاسخ می‌شود، به صورت یکپارچه مورد سنجش قرار گرفته است. در آزمایش‌های این بخش صرفا تاریخچه گفتگو و جملات نامزد دانش به مدل داده شده است و مدل بایستی که پاسخ درست را تولید کند. در این آزمایش یکی از چالش‌های اساسی این است که ممکن است بخش انتخاب دانش جمله اشتباهی را به بخش تولید پاسخ تحویل دهد. در تمامی آزمایش‌های این بخش مدل شش لایه با اندازه نهان ۲۵۶ متعلق به بخش 

که از تابع خطی در هنگام آموزش خود استفاده کرده است،‌ به عنوان مدل انتخاب دانش استفاده شده است.

\section{حالت تک جمله}
در این حالت جمله نامزد انتخاب شده توسط مدل انتخاب دانش به بخش تولید پاسخ ورودی داده می‌شود. 



